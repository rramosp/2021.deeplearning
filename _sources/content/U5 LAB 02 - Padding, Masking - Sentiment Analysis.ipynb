{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5.2 - Padding - Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/2021.deeplearning/main/content/init.py\n",
    "import init; init.init(force_download=False); init.get_weblink() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.lib.rlxmoocapi import submit, session\n",
    "import inspect\n",
    "session.LoginSequence(endpoint=init.endpoint, course_id=init.course_id, lab_id=\"L05.02\", varname=\"student\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print (\"setting tensorflow version in colab\")\n",
    "    %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic required libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The aim of this lab is to build a system for sentiment analysis on a dataset of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consist on passenger's reviews of U.S. airlines: https://www.kaggle.com/crowdflower/twitter-airline-sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('local/data/Tweets.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text airline_sentiment\n",
       "0                    @VirginAmerica What @dhepburn said.           neutral\n",
       "1      @VirginAmerica plus you've added commercials t...          positive\n",
       "2      @VirginAmerica I didn't today... Must mean I n...           neutral\n",
       "3      @VirginAmerica it's really aggressive to blast...          negative\n",
       "4      @VirginAmerica and it's a really big bad thing...          negative\n",
       "...                                                  ...               ...\n",
       "14635  @AmericanAir thank you we got on a different f...          positive\n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...          negative\n",
       "14637  @AmericanAir Please bring American Airlines to...           neutral\n",
       "14638  @AmericanAir you have my money, you change my ...          negative\n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...           neutral\n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples=2363\n",
      "Number of negative samples=9178\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#Remove neutral class\n",
    "data = data[data.airline_sentiment != \"neutral\"]\n",
    "\n",
    "#text normalization\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x:re.sub('@[^\\s]+','',x)))#remove the name of the airline\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "Np = np.sum(data['airline_sentiment'].values == 'positive')\n",
    "Nn = np.sum(data['airline_sentiment'].values == 'negative')\n",
    "print(f'Number of positive samples={Np}')\n",
    "print(f'Number of negative samples={Nn}')\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus youve added commercials to the experienc...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>its really aggressive to blast obnoxious ente...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>seriously would pay 30 a flight for seats tha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes nearly every time i fly vx this ear worm ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14633</th>\n",
       "      <td>my flight was cancelled flightled leaving tom...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>right on cue with the delays</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>thank you we got on a different flight to chi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>leaving over 20 minutes late flight no warnin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>you have my money you change my flight and do...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text airline_sentiment\n",
       "1       plus youve added commercials to the experienc...          positive\n",
       "3       its really aggressive to blast obnoxious ente...          negative\n",
       "4                and its a really big bad thing about it          negative\n",
       "5       seriously would pay 30 a flight for seats tha...          negative\n",
       "6       yes nearly every time i fly vx this ear worm ...          positive\n",
       "...                                                  ...               ...\n",
       "14633   my flight was cancelled flightled leaving tom...          negative\n",
       "14634                       right on cue with the delays          negative\n",
       "14635   thank you we got on a different flight to chi...          positive\n",
       "14636   leaving over 20 minutes late flight no warnin...          negative\n",
       "14638   you have my money you change my flight and do...          negative\n",
       "\n",
       "[11541 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/julian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/julian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "all_sentences = data['text'].values\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('')\n",
    "\n",
    "for i in range(len(all_words)):  \n",
    "    all_words[i] = [w for w in all_words[i] if (w not in stop_words) and (not w.isdigit())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "all_words is a list with all the tweets that are going to be used to train the model. Create a function 'get_preprocessed_seq' that build and apply a Tokenizer to the a list like all_words. Tokenizer must define a dictionary of 2000 words (remeber that position 0 is reserved). Once the sentences are tokenized, take into account that the length of every tweet is different so before they can be passed to the training step, the tweets must be **padded** in order to provide them with equal length. The function 'get_preprocessed_seq' must return the Tokenizer object and the padded dataset.\n",
    "\n",
    "Review the [padding function](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) in the preprocessing module of keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_seq(text_list):\n",
    "    ...\n",
    "    return tokenizer, Xdata "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "The previous step add 0's to some tweets in order to set them with the same length. Now it is necessary to define a model that be able to discard those 0's. Review the masking layer and masking option of the embedding layer of keras. Take a look to the TensorFlow [documentation](https://www.tensorflow.org/guide/keras/masking_and_padding).\n",
    "\n",
    "Create a function to define a RNN architecture for the sentiment analysis problem. Use the Embedding layer and its masking option to discard the 0's added during padding step. The architecture must consist of a RNN layer with a 'cells_number' of neurons, a dense hidden layer of 10 neurons and the output layer. Include a Dropout layer in between the dense layers with a drop rate of 0.3 . The type of RNN layer must be defined through a 'layer_type' parameter; the embedding dimension must also be set as an input parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recurrent_Model(cells_number = 10, layer_type='SimpleRNN',Embeb_dim=32):#Options for layer_type: 'SimpleRNN', 'LSTM', 'GRU'\n",
    "    model = ...\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the model: \n",
    "\n",
    "**Warning**: Run this part only if you have already passed Tasks 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "tokenizer, NewX = get_preprocessed_seq(all_words)\n",
    "\n",
    "y = data['airline_sentiment'].values\n",
    "Encoder = LabelEncoder()\n",
    "Y = Encoder.fit_transform(y)\n",
    "#Y = Encoder.transform(y)\n",
    "\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(NewX, Y, test_size=0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def especi_score(y_te,y_pred):\n",
    "    Ns = np.sum(y_te == 0)\n",
    "    return np.sum(y_te[y_te == 0] == y_pred[y_te == 0])/Ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = np.zeros((3,3))\n",
    "especificity = np.zeros((3,3))\n",
    "accuracy = np.zeros((3,3))\n",
    "for i, embed_dim in enumerate([32,64,128]):\n",
    "    for j,cells in enumerate([32,64,128]):\n",
    "        model = Recurrent_Model(cells_number= cells, layer_type='LSTM', Embeb_dim=embed_dim)\n",
    "        #opt = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        model.fit(X_tr, y_tr, validation_split=0.1,batch_size=32, epochs=10, verbose=1)\n",
    "        y_pred = np.round(model.predict(X_te))\n",
    "        sensitivity[i,j] = recall_score(y_te,y_pred)\n",
    "        accuracy[i,j] = accuracy_score(y_te,y_pred)\n",
    "        especificity[i,j] = especi_score(y_te,y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.lib.DataPreparationRNN import Plot_sentiment_performance\n",
    "Plot_sentiment_performance(sensitivity,accuracy,especificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Create a similar architecture to that of task 2, but in this case use global vectors (GloVe) from gensim library to set the Embedding weights. Set the Embedding layer as non trainable. If there is any missing word in the pre-trained GloVes, you can use the token 'unk' instead.\n",
    "\n",
    "**Note**: Take care on the tokenization of the words. Keras tokenizer does not assign the zero value to any word because of padding purposes. Make sure that the order of the vectors in the GloVe embedding matrix corresponds with the indexes in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to get the pre-trained word embedding vectors\n",
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to provide the grader with the word embedding vectors\n",
    "import pickle\n",
    "with open('GloVe.pkl', 'wb') as output:\n",
    "    pickle.dump(glove_vectors, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recurrent_Model_TF(tokenizer,glove_vectors,cells_number = 10, layer_type='SimpleRNN'):#Options for layer_type: 'SimpleRNN', 'LSTM', 'GRU'\n",
    "    model = ...\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the model: \n",
    "\n",
    "**Warning**: Run this part only if you have already passed Tasks 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = np.zeros((3))\n",
    "especificity = np.zeros((3))\n",
    "accuracy = np.zeros((3))\n",
    "for j,cells in enumerate([32,64,128]):\n",
    "    model = Recurrent_Model_TF(tokenizer, glove_vectors, cells_number = cells, layer_type='LSTM')\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(X_tr, y_tr, validation_split=0.1,batch_size=32, epochs=10, verbose=1)\n",
    "    y_pred = np.round(model.predict(X_te))\n",
    "    sensitivity = recall_score(y_te,y_pred)\n",
    "    accuracy[j] = accuracy_score(y_te,y_pred)\n",
    "    especificity[j] = especi_score(y_te,y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "X = np.arange(3)\n",
    "ax.bar(X + 0.00, accuracy, color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, sensitivity, color = 'g', width = 0.25)\n",
    "ax.bar(X + 0.50, especificity, color = 'r', width = 0.25)\n",
    "ax.set_xticks([0.25, 1.25, 2.25])\n",
    "ax.set_xticklabels(['32','64', '128'])\n",
    "ax.set_title('Performance')\n",
    "ax.set_xlabel('Number of cells')\n",
    "ax.legend(labels=['accuracy','sensitivity','especificity'],bbox_to_anchor=(1.1, 1.05))\n",
    "print('Best accuracy= {}'.format(np.max(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Create a function similar to that of task 3, but use a Conv1D layer instead of the LSTM one. The output of the Conv1D layer must keep the dimension related to the number of words per sentence unchanged. The number of filters (kernels) must be a functions' input parameter and define the kernels size in order for the model to use trigrams.  Let the rest of parameters with their default values. Use the GloVe embedding weights from the former task. To complete the architecture and shape correctly the tensors, you must use a GlobalMaxPooling1D layer after the CNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Con1D_Model_TF(tokenizer, glove_vectors, filters = 10):\n",
    "    ...\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T4');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the model: \n",
    "\n",
    "**Warning**: Run this part only if you have already passed Tasks 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = np.zeros((3))\n",
    "especificity = np.zeros((3))\n",
    "accuracy = np.zeros((3))\n",
    "for j,cells in enumerate([6,12,24]):\n",
    "    model = Con1D_Model_TF(tokenizer, glove_vectors, filters = cells)\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(X_tr, y_tr, validation_split=0.1,batch_size=32, epochs=10, verbose=1)\n",
    "    y_pred = np.round(model.predict(X_te))\n",
    "    sensitivity = recall_score(y_te,y_pred)\n",
    "    accuracy[j] = accuracy_score(y_te,y_pred)\n",
    "    especificity[j] = especi_score(y_te,y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "X = np.arange(3)\n",
    "ax.bar(X + 0.00, accuracy, color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, sensitivity, color = 'g', width = 0.25)\n",
    "ax.bar(X + 0.50, especificity, color = 'r', width = 0.25)\n",
    "ax.set_xticks([0.25, 1.25, 2.25])\n",
    "ax.set_xticklabels(['6','12', '24'])\n",
    "ax.set_title('Performance')\n",
    "ax.set_xlabel('Number of filters')\n",
    "ax.legend(labels=['accuracy','sensitivity','especificity'],bbox_to_anchor=(1.1, 1.05))\n",
    "print('Best accuracy= {}'.format(np.max(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
