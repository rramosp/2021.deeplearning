{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3.2 - Low level `tensorflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labs require a `tensorflow` version lower than the default one used in Google Colab. Run the following cell to downgrade TensorFlow accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def downgrade_tf_version():\n",
    "    os.system(\"!yes | pip uninstall -y tensorflow\")\n",
    "    os.system(\"!yes | pip install tensorflow==2.12.0\")\n",
    "    os.kill(os.getpid(), 9)\n",
    "downgrade_tf_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/2021.deeplearning/main/content/init.py\n",
    "import init; init.init(force_download=False); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from local.lib.rlxmoocapi import submit, session\n",
    "import inspect\n",
    "session.LoginSequence(endpoint=init.endpoint, course_id=init.course_id, lab_id=\"L03.02\", varname=\"student\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Obtain layer output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPLETE** the following function so that, when given a TF `model` and input  `X` returns the output of the model at layer `layer_name` when feeding `X` to the model.\n",
    "\n",
    "\n",
    "You **MUST RETURN** a `numpy` array, **NOT** a tensor.\n",
    "\n",
    "**HINT**: Use the [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) class like in the functional API with `outputs` from the desired layer.\n",
    "\n",
    "**CHALLENGE**: Solve it with a single line of code (not counting the `import`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_at_layer(X, model, layer_name):\n",
    "    from tensorflow.keras.models import Model\n",
    "    r = ... # YOUR CODE HERE\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check your answer manually. With the following model and weights below you should get this answer:\n",
    "\n",
    "\n",
    "        >> output_at_layer(X, model, \"layer_A\")\n",
    "        array([[0.91274303, 0.69886017, 0.8832942 ],\n",
    "               [0.9311633 , 0.7634138 , 0.8924969 ],\n",
    "               [0.85661894, 0.5696809 , 0.8091405 ],\n",
    "               [0.8952345 , 0.6803274 , 0.8326857 ]], dtype=float32)\n",
    "\n",
    "        >> output_at_layer(X, model, \"layer_B\")\n",
    "        array([[0.87063193, 0.8240411 ],\n",
    "               [0.8774254 , 0.83376545],\n",
    "               [0.84875023, 0.7963983 ],\n",
    "               [0.86286545, 0.81590414]], dtype=float32)\n",
    "\n",
    "        >> output_at_layer(X, model, \"layer_C\")\n",
    "        array([[0.8959839 , 0.65980244],\n",
    "               [0.9032545 , 0.66435313],\n",
    "               [0.8733646 , 0.646801  ],\n",
    "               [0.8883195 , 0.6559416 ]], dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n1,n2,n3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n1, name=\"layer_A\", activation=\"tanh\", input_dim=2))\n",
    "    model.add(Dense(n2, name=\"layer_B\", activation=\"sigmoid\"))\n",
    "    model.add(Dense(n3, name=\"layer_C\", activation=\"linear\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "w = [np.array([[0.3336241 , 0.26024526, 0.37238857],\n",
    "               [0.6344426 , 0.67660165, 0.31070882]], dtype=np.float32),\n",
    "     np.array([0.97280777, 0.3447949 , 0.91722184], dtype=np.float32),\n",
    "     \n",
    "     np.array([[0.12999585, 0.31851983],\n",
    "               [0.7763866 , 0.8777575 ],\n",
    "               [0.99977154, 0.65771514]], dtype=np.float32),\n",
    "     np.array([0.36222705, 0.05885772], dtype=np.float32),\n",
    "\n",
    "     np.array([[0.75918376, 0.02541249],\n",
    "               [0.21730722, 0.45021895]], dtype=np.float32),\n",
    "     np.array([0.05594416, 0.26667854], dtype=np.float32)]\n",
    "\n",
    "\n",
    "X = np.array([[0.9269997 , 0.41239464],\n",
    "              [0.8461177 , 0.64935404],\n",
    "              [0.27092433, 0.34251866],\n",
    "              [0.22509325, 0.6301328 ]], dtype=np.float32)\n",
    "\n",
    "\n",
    "model=get_model(3,2,2)\n",
    "model.set_weights(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_at_layer(X, model, \"layer_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which corresponds to a tanh activation from the input data\n",
    "np.tanh(X.dot(w[0])+w[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_at_layer(X, model, \"layer_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which corresponds to a sigmoid activation from the output of layer A\n",
    "sigm = lambda x: 1/(1+np.exp(-x))\n",
    "sigm(output_at_layer(X, model, \"layer_A\").dot(w[2])+w[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_at_layer(X, model, \"layer_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which corresponds to a linear activation from the output of layer B\n",
    "output_at_layer(X, model, \"layer_B\").dot(w[-2])+w[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registra tu soluci√≥n en linea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Implement batch normalization\n",
    "\n",
    "Observe how we create a **ONE LAYER** model with **TANH** activation and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim, n):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, name=\"layer_A\", activation=\"tanh\", input_dim=input_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we manually initialize it with random weights and apply it to some random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 4 , n 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = np.random.randint(3)+2\n",
    "n = np.random.randint(5)+5\n",
    "X = np.random.random((6,input_dim)).astype(np.float32)\n",
    "print (\"input_dim\", input_dim, \", n\", n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.080421  , 0.9186753 , 1.4783407 , 0.5158316 , 0.7339326 ],\n",
       "       [1.047442  , 0.8679704 , 1.4229709 , 0.51811504, 0.6608341 ],\n",
       "       [1.0745426 , 0.90660375, 1.4581381 , 0.5141438 , 0.73286605],\n",
       "       [1.0549638 , 0.88741153, 1.3028177 , 0.48024043, 0.66753477],\n",
       "       [1.0836991 , 0.90194285, 1.5070624 , 0.5117415 , 0.7579496 ],\n",
       "       [1.0492579 , 0.88168395, 1.3504746 , 0.5018039 , 0.6871215 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = get_model(input_dim=input_dim, n=n)\n",
    "model.set_weights([np.random.random(i.shape) for i in model.get_weights()])\n",
    "model(X).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can extract the weights of the dense layer and the batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 5), (5,), (5,), (5,), (5,))"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "W, b, gamma, beta, moving_mean, moving_var = model.get_weights()\n",
    "W.shape, b.shape, beta.shape, moving_mean.shape, moving_var.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPLETE** the following function **WITHOUT USING TENSORFLOW** such that you get the same output as the model above, given the input and the weights.\n",
    "\n",
    "In specific, the Dense layer output must be\n",
    "\n",
    "$$A = \\text{tanh}(XW+b)$$\n",
    "\n",
    "And the batch normalization layer after that output is\n",
    "\n",
    "$$\\frac{A-m_\\mu}{\\sqrt{m_\\sigma+\\varepsilon}}\\gamma + \\beta$$\n",
    "\n",
    "You **MUST RETURN** a `numpy` array, **NOT** a tensor.\n",
    "\n",
    "**CHALLENGE**: Solve it with one single line of Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(X, W, b, gamma, beta, moving_mean, moving_var, epsilon=1e-3):\n",
    "    r = ... # YOUR CODE HERE\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check manually your code, the output should be the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model(X, W, b, gamma, beta, moving_mean, moving_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registra tu soluci√≥n en linea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Compute the Hessian\n",
    "\n",
    "Complete the function below so that computes the Hessian of a function with respect to a set of variables. Remember that the Hessian is the matrix with all combinations of double partial derivatives. See [https://en.wikipedia.org/wiki/Hessian_matrix](https://en.wikipedia.org/wiki/Hessian_matrix)\n",
    "\n",
    "\n",
    "The arguments for your code below:\n",
    "\n",
    "- `expression_fn`: is a Python function that, when executed, will return a Tensor depending on the variables in `svars`.\n",
    "- `svars`: a list of $n$ `tf.Variable`s against which  the derivatives are to be taken.\n",
    "\n",
    "The result:\n",
    "\n",
    "- a `numpy` array of dimension $n\\times n$, containing in each position the value of the corresponding double derivative evaluated with the value attached to each variable in `svars`.\n",
    "\n",
    "See the example call below to understand what you have to produce.\n",
    "\n",
    "**NOTE**: Observe that `expression_fn` is a function that you **must call** inside some `GradientTape` to obtain the expresion. This needs to be done this way because `GradientTape` needs to _watch_ how expressions are built to be able to access the computational graph and compute the gradient. This is a technique which is very often used in Tensorflow.\n",
    "\n",
    "\n",
    "\n",
    "**WARN**: You cannot use `tf.hessian` or `GradientTape.jacobian` or `sympy`. Do not use the name `hessian` to name any variable within your code.\n",
    "\n",
    "**HINT 1**: use a `GradientTape` inside another `GradientTape`.\n",
    "\n",
    "**HINT 2**: use `unconnected_gradients=tf.UnconnectedGradients.ZERO` as argument to `GradientTape.gradient` to have the variables not participating in an expresion result in gradient zero. For instance with $f=xy$, we have $\\frac{\\partial f}{\\partial y\\partial y}=0$, since $\\frac{\\partial f}{\\partial y}=1$. Or if we have $f=x$ we have $\\frac{\\partial f}{\\partial y\\partial x}=0$, since $\\frac{\\partial f}{\\partial y}=0$. If you do not include this argument, Tensorflow will return these values as `None` and you would have to fix them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_double_derivatives(expression_fn,svars):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    result = ... \n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check your code. The following expression\n",
    "\n",
    "$$f = 2xy^2 + 3x\\cos{y}$$\n",
    "\n",
    "has as first derivatives:\n",
    "\n",
    "- $\\frac{\\partial f }{\\partial x} = 2y^2 +3\\cos{y}$\n",
    "- $\\frac{\\partial f }{\\partial y} = 4xy - 3x\\sin{y}$\n",
    "\n",
    "and as second derivatives:\n",
    "\n",
    "- $\\frac{\\partial f }{\\partial x \\partial x} = 0$\n",
    "- $\\frac{\\partial f }{\\partial x \\partial y} = 4y - 3\\sin{y}$\n",
    "- $\\frac{\\partial f }{\\partial y \\partial x} = 4y - 3\\sin{y}$\n",
    "- $\\frac{\\partial f }{\\partial y \\partial y} = 4x - 3x\\cos{y}$\n",
    "\n",
    "which, when evaluated at $x=2$ and $y=-3$ yields\n",
    "\n",
    "    [[  0     ,  -11.58],\n",
    "     [ -11.58 ,   13.94]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2, dtype=tf.float32)\n",
    "y = tf.Variable(-3, dtype=tf.float32)\n",
    "expr = lambda: 2*x*y**2 + 3*x*tf.cos(y)\n",
    "\n",
    "get_double_derivatives(expr, [x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registra tu soluci√≥n en linea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
