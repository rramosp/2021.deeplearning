{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3.1 - TF model subclassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labs require a `tensorflow` version lower than the default one used in Google Colab. Run the following cell to downgrade TensorFlow accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def downgrade_tf_version():\n",
    "    os.system(\"!yes | pip uninstall -y tensorflow\")\n",
    "    os.system(\"!yes | pip install tensorflow==2.12.0\")\n",
    "    os.kill(os.getpid(), 9)\n",
    "downgrade_tf_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/2021.deeplearning/main/content/init.py\n",
    "import init; init.init(force_download=False); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from local.lib.rlxmoocapi import submit, session\n",
    "import inspect\n",
    "session.LoginSequence(endpoint=init.endpoint, course_id=init.course_id, lab_id=\"L03.01\", varname=\"student\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "from sklearn.datasets import *\n",
    "from local.lib import mlutils\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**A multilayer perceptron**\n",
    "\n",
    "assuming $n$ layers, the output at layer $i$\n",
    "\n",
    "$$\\mathbf{a}_i = \\text{activation}(\\mathbf{a}_{i-1} \\cdot \\mathbf{W}_i + \\mathbf{b}_i)$$\n",
    "\n",
    "at the first layer\n",
    "\n",
    "$$\\mathbf{a}_0 = \\text{activation}(\\mathbf{X} \\cdot \\mathbf{W}_0 + \\mathbf{b}_0)$$\n",
    "\n",
    "and the layer prediction is the output of the last layer:\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{a}_{n-1}$$ \n",
    "\n",
    "with $\\text{activation}$ being an activation function, such as $\\text{sigmoid}(z) = \\frac{1}{1+e^{-z}}$, $\\text{tanh}$, $\\text{ReLU}$, etc.\n",
    "\n",
    "\n",
    "**Cost (with regularization)**\n",
    "\n",
    "\n",
    "$$J(\\mathbf{b}_1, b_2, \\mathbf{W}_1, \\mathbf{W}_2) = \\frac{1}{m}\\sum_{i=0}^{m-1} (\\hat{y}-y)^2 + \\lambda \\sum_{i=0}^{n-1} \\bigg[ \\| \\mathbf{b}_i\\|^2 + \\|\\mathbf{W}_i\\|^2 \\bigg]$$\n",
    "\n",
    "\n",
    "$\\lambda$ regulates the participation of the regularization terms. Given a vector or matrix $\\mathbf{T}$, its squared norm is denoted by $||\\mathbf{T}||^2 \\in \\mathbb{R}$ and it's computed by squaring all its elements and summing them all up. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Model `build` \n",
    "\n",
    "Observe the class template below which is used to build a multilayer perceptron with a specific number of layers. In the constructor.\n",
    "\n",
    "- `neurons` must be a list of integers specifying the number of neurons of each hidden layer and the output layer.\n",
    "- `activations` must be a list of strings specifying  the activations of the neurons of each layer.\n",
    "\n",
    "Both `neurons` and `activations` must have the same number of elements. Observe how in the class constructor (`__init__`) we check for this and transform the list of activation strings to actual TF funcions.\n",
    "\n",
    "**YOU MUST** complete the `build` method in the class below so that `self.W` and `self.b` contain a list of tensors with randomly initialized weights for each layer. Create the weights by calling the `self.add_weights` function for each layer, both for the weights (add them to list `self.W`) and the biases (add them to list `b`). Call `self.add_weights` with parameters `initializer='random_normal', trainable=True, dtype=tf.float32`.\n",
    "\n",
    "Note that the shape of the first layer weights are not known until the `build` method is called which is when the `input_shape` for the input data is known. For instance, the following invokations\n",
    "\n",
    "\n",
    "    >> mlp = MLP_class(neurons=[10,5,1], activations=[\"tanh\",\"tanh\", \"sigmoid\"])\n",
    "    >> mlp.build([None, 2])\n",
    "    >> print (\"W shapes\", [i.shape for i in mlp.W])\n",
    "    \n",
    "should produce the following output\n",
    "    \n",
    "    W shapes [TensorShape([2, 10]), TensorShape([10, 5]), TensorShape([5, 1])]\n",
    "    b shapes [TensorShape([10]), TensorShape([5]), TensorShape([1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(neurons, activations, reg=0.):\n",
    "\n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras.activations import relu, sigmoid, tanh, linear\n",
    "    import numpy as np\n",
    "    import tensorflow as tf    \n",
    "\n",
    "    class MLP_class(Model):\n",
    "        def __init__(self, neurons, activations, reg=0.):\n",
    "            super().__init__()\n",
    "            self.activation_map = {\"linear\": linear, \"relu\": relu, \"tanh\":tanh, \"sigmoid\": sigmoid}\n",
    "            \n",
    "            assert len(neurons)==len(activations), \\\n",
    "                        \"must have the same number of neurons and activations\"\n",
    "                \n",
    "            assert np.alltrue([i in self.activation_map.keys() for i in activations]), \\\n",
    "                                \"activation string not recognized\"\n",
    "            \n",
    "            self.neurons = neurons\n",
    "            self.reg = reg\n",
    "            self.activations = [self.activation_map[i] for i in activations]\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.W = []\n",
    "            self.b = []\n",
    "\n",
    "            ... # YOUR CODE HERE\n",
    "\n",
    "            \n",
    "    return MLP_class(neurons, activations, reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test manually your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(neurons=[10,5,1], activations=[\"tanh\",\"tanh\", \"sigmoid\"])\n",
    "mlp.build([None, 2])\n",
    "print (\"W shapes\", [i.shape for i in mlp.W])\n",
    "print (\"b shapes\", [i.shape for i in mlp.b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registra tu soluci√≥n en linea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model `call` \n",
    "\n",
    "Complete the `call` method below so that it computes the output of the configured MLP with the input `X` as\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{a}_{n-1}$$ \n",
    "\n",
    "as described above. Use `self.W`, `self.b` and `self.activations` as constructed previously on the `build` and `__init__` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP2(neurons, activations, reg=0.):\n",
    "    \n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras.activations import relu, sigmoid, tanh, linear   \n",
    "\n",
    "    class MLP_class(Model):\n",
    "        def __init__(self, neurons, activations, reg=0.):\n",
    "            super().__init__()\n",
    "            self.activation_map = {\"linear\": linear, \"relu\": relu, \"tanh\":tanh, \"sigmoid\": sigmoid}\n",
    "            \n",
    "            assert len(neurons)==len(activations), \\\n",
    "                        \"must have the same number of neurons and activations\"\n",
    "                \n",
    "            assert np.alltrue([i in self.activation_map.keys() for i in activations]), \\\n",
    "                                \"activation string not recognized\"\n",
    "            \n",
    "            self.neurons = neurons\n",
    "            self.reg = reg\n",
    "            self.activations = [self.activation_map[i] for i in activations]\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.W = []\n",
    "            self.b = []\n",
    "\n",
    "            ... # YOUR CODE HERE (copy from previous task )\n",
    "            \n",
    "        @tf.function\n",
    "        def call(self, X):\n",
    "            a = ... # YOUR CODE HERE\n",
    "            return s\n",
    "        \n",
    "    return MLP_class(neurons, activations, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test manually your code, the following two cells must return the same value everytime you execute them. Observe your MLP will initialize to different random weights each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random(size=(4,2))\n",
    "neurons = [3,2]\n",
    "mlp = MLP2(neurons=[3,2], activations=[\"linear\", \"sigmoid\"])\n",
    "mlp(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigm = lambda x: 1/(1+np.exp(-x))\n",
    "W = [i.numpy() for i in mlp.W]\n",
    "b = [i.numpy() for i in mlp.b]\n",
    "sigm((X.dot(W[0])+b[0]).dot(W[1])+b[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registra tu soluci√≥n en linea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Loss function\n",
    "\n",
    "Complete the `loss` method below so that it computes the loss of the `MLP` given predictions `y_pred` (as the output of the network) and desired output `y_true`.\n",
    "\n",
    "$$J(\\mathbf{b}_1, b_2, \\mathbf{W}_1, \\mathbf{W}_2) = \\frac{1}{m}\\sum_{i=0}^{m-1} (\\hat{y}-y)^2 + \\lambda \\sum_{i=0}^{n-1} \\bigg[ \\| \\mathbf{b}_i\\|^2_{mean} + \\|\\mathbf{W}_i\\|^2_{mean} \\bigg]$$\n",
    "\n",
    "\n",
    "observe the regularization term $\\lambda$ which is stored as `self.reg` in your class.\n",
    "\n",
    "For any weight or bias $\\mathbf{k}$, the expression $\\| \\mathbf{k}\\|^2_{mean}$ is the mean of all its elements squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP3(neurons, activations, reg=0.):\n",
    "    \n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras.activations import relu, sigmoid, tanh, linear   \n",
    "\n",
    "    class MLP_class(Model):\n",
    "        def __init__(self, neurons, activations, reg=0.):\n",
    "            super().__init__()\n",
    "            self.activation_map = {\"linear\": linear, \"relu\": relu, \"tanh\":tanh, \"sigmoid\": sigmoid}\n",
    "            \n",
    "            assert len(neurons)==len(activations), \\\n",
    "                        \"must have the same number of neurons and activations\"\n",
    "                \n",
    "            assert np.alltrue([i in self.activation_map.keys() for i in activations]), \\\n",
    "                                \"activation string not recognized\"\n",
    "            \n",
    "            self.neurons = neurons\n",
    "            self.reg = reg\n",
    "            self.activations = [self.activation_map[i] for i in activations]\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.W = []\n",
    "            self.b = []\n",
    "\n",
    "            ... # YOUR CODE HERE (copy from previous task)\n",
    "            \n",
    "        @tf.function\n",
    "        def call(self, X):\n",
    "            a = ... # YOUR CODE HERE (copy from previous task)\n",
    "            return s\n",
    "\n",
    "        \n",
    "        @tf.function\n",
    "        def loss(self, y_true, y_pred):\n",
    "            r = ... # YOUR CODE HERE\n",
    "            return ...\n",
    "        \n",
    "    return MLP_class(neurons, activations, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test manually your code, the following two cells must return the same value everytime you execute them. Observe your MLP will initialize to different random weights each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random(size=(4,2)).astype(np.float32)\n",
    "y_true = np.random.randint(2, size=(len(X),1)).astype(np.float32)\n",
    "neurons = [3,2]\n",
    "mlp = MLP3(neurons=[3,1], activations=[\"linear\", \"sigmoid\"], reg=0.2)\n",
    "mlp.loss(mlp(X), y_true).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigm = lambda x: 1/(1+np.exp(-x))\n",
    "W = [i.numpy() for i in mlp.W]\n",
    "b = [i.numpy() for i in mlp.b]\n",
    "y_pred = sigm((X.dot(W[0])+b[0]).dot(W[1])+b[1])\n",
    "((y_pred-y_true)**2).mean() + mlp.reg * np.sum([(i**2).numpy().mean() for i in mlp.W+mlp.b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registra tu soluci√≥n en linea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student.submit_task(namespace=globals(), task_id='T3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!!\n",
    "\n",
    "now you can try your class with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(200, noise=.35)\n",
    "X, y = X.astype(np.float32), y.astype(np.float32).reshape(-1,1)\n",
    "plt.scatter(X[:,0][y[:,0]==0], X[:,1][y[:,0]==0], color=\"red\", label=\"class 0\")\n",
    "plt.scatter(X[:,0][y[:,0]==1], X[:,1][y[:,0]==1], color=\"blue\", label=\"class 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create MLP and train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP3(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"], reg=0.0)\n",
    "mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=mlp.loss,\n",
    "           metrics=[tf.keras.metrics.mae, tf.keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/no_regularization\")\n",
    "mlp.fit(X,y, epochs=400, batch_size=16, verbose=0, \n",
    "        callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observe the accuracy and classification frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lambda X: (mlp.predict(X)[:,0]>0.5).astype(int)\n",
    "mlutils.plot_2Ddata_with_boundary(predict, X, y.reshape(-1));\n",
    "plt.title(\"accuracy %.2f\"%np.mean(predict(X)==y.reshape(-1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularization must work!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/with_regularization\")\n",
    "mlp = MLP3(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"], reg=0.005)\n",
    "\n",
    "mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=mlp.loss,\n",
    "           metrics=[tf.keras.metrics.mae, tf.keras.metrics.binary_accuracy])\n",
    "\n",
    "mlp.fit(X,y, epochs=400, batch_size=10, verbose=0, callbacks=[tensorboard_callback])\n",
    "mlutils.plot_2Ddata_with_boundary(predict, X, y.reshape(-1))\n",
    "plt.title(\"accuracy %.2f\"%np.mean(predict(X)==y.reshape(-1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and inspect tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
